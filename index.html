<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Add jQuery library -->
    <script type="text/javascript" src="lib/jquery-1.8.2.min.js"></script>
    <!-- Add mousewheel plugin (this is optional) -->
    <script type="text/javascript" src="lib/jquery.mousewheel-3.0.6.pack.js"></script>
    <!-- Add fancyBox main JS and CSS files -->
    <script type="text/javascript" src="source/jquery.fancybox.js?v=2.1.3"></script>
    <link rel="stylesheet" type="text/css" href="source/jquery.fancybox.css?v=2.1.2" media="screen" />
    <!-- javascript function-->
    <script type="text/javascript">
    $(document).ready(function() {
    $('.fancybox').fancybox();
    $(".fancybox-effects-c").fancybox({
      wrapCSS    : 'fancybox-custom',
      closeClick : true,
      openEffect : 'none',
      helpers : {
        title : {
          type : 'inside'
        },
        overlay : {
          css : {
            'background' : 'rgba(238,238,238,0.85)'
          }
        }
      }
        });
    });
    </script>
    <script type="text/javascript">
    function display(id){  
      var traget=document.getElementById(id);  
      if(traget.style.display=="none"){  
          traget.style.display="";  
      }else{  
          traget.style.display="none";  
        }  
        }  
        </script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>Yang Liu @SYSU</title>
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <!-- Custom fonts for this template
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans:100,200,300,400,500,600,700,800,900"
      rel="stylesheet"> -->
    <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i"
      rel="stylesheet"> -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="shortcut icon" href="favicon.ico">
    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">
    <link href="css/index.css" rel="stylesheet">
</head>

<body id="page-top">

  <!-- Copyright ? 2008. Spidersoft Ltd -->
  <style>
  </style>
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none"></span>
      <span class="d-none d-lg-block">
        <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="./img/ArYanR2024.jpg" alt="">
      </span>
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#about">About Me</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#News">News</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#activities">Activities</a>
        </li>          
      </ul>
    </div>
  </nav>

  <div class="container-fluid p-0">
    <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
      <div class="my-auto">
        <h2 class="mb-0">Yang Liu (刘阳)</h2>
        <div class="mb-5"> 
             IEEE Member, CCF Member, CAAI Member, CSIG Member<br/>
            <b>Email</b>: liuy856@mail.sysu.edu.cn<br/>
            <b>Office</b>:  School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China<br/>
        </div>
        <p class="mb-5">I am currently a research associate professor at <a href="http://www.sysu-hcp.net/" target="_blank">HCP Lab</a>, <a href="http://sdcs.sysu.edu.cn/" target="_blank">School of Computer Science and Engineering</a>, 
            <a href="http://www.sysu.edu.cn/" target="_blank">Sun Yat-sen University</a>, working with <a href="http://www.linliang.net/" target="_blank">Prof. Liang Lin</a>. From 2019-2021, I worked as a posdoctoral fellow at School of Computer Science and Engineering, Sun Yat-sen University.
               I obtained my Ph.D degree of Telecommunications and Information Systems in June 2019 at <a href="http://www.phoenix-eye.com/" target="_blank">Phoenix-eye (XD145) Lab</a>, <a href="http://ste.xidian.edu.cn/" target="_blank">School of Telecommunications Engineering</a>,
              <a href="https://www.xidian.edu.cn/" target="_blank">Xidian University</a>, advised by <a href="http://web.xidian.edu.cn/zhylu/" target="_blank">Prof. Zhaoyang Lu</a>, <a href="https://web.xidian.edu.cn/jingli/" target="_blank">Prof. Jing Li</a> and <a href="https://teacher.nwpu.edu.cn/taoyang.html" target="_blank">Prof. Tao Yang</a>. I serve as the reviewer of IEEE T-PAMI, T-IP, T-NNLS, T-MM, T-CSVT, CVPR, ICCV, AAAI, ACM MM, ECCV, etc.<br/>
             <a href="https://www.sysu-hcp.net/faculty/liuyang.html" target="_blank">[Chinese Version]</a>
            <ul>
            <li>
               <b>Computer Vision</b>: Cross-modal Reasoning, Spatial-temporal Representation Learning
            </li>
            <li>
               <b>Machine Learning</b>: Causality Inference, Self-supervised Learning, Transfer Learning
            </li>
             <li>
               <b>Embodied AI</b>: Embodied Interaction, Embodied Manipulation, Robotic Control
            </li>
                     </ul>  
        </p>
        
              <!--<div class="my-auto">
             <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            Guangdong Basic and Applied Basic Research Foundation, ¥ 100K, 2023-2026, PI</li>
        </ul> 
                    <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            Science and Technology Projects in Guangzhou, ¥ 50K, 2022-2024, PI</li>
        </ul>  
          <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            National Key Research and Development Program of China, ¥ 5,000K, 2021-2025, Key Participant</li>
        </ul>  
        <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            National Natural Science Foundation of China, ¥ 300K, 2021-2024, PI</li>
        </ul>  
        <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            Guangdong Basic and Applied Basic Research Foundation, ¥ 100K, 2021-2024, PI</li>
        </ul> 
       <ul class="fa-ul mb-0">  
        <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            General Program of China Postdoctoral Science Foundation, ¥ 80K, 2020-2022, PI</li>
        </ul> 
       <ul class="fa-ul mb-0">
        <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            Fundamental Research Funds for the Central Universities, ¥ 120K, 2020-2022, PI</li>
        </ul>   
              </div>-->
        
<div class="my-auto">
            <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            CCF ChinaSoft 2023 Robotic Big Model and Embodied Intelligence Challenge, Third Prize, 2023.</li>
        </ul>
        <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            The Third Guangdong Province Young Computer Science Academic Show, First Prize, 2023.</li>
        </ul>
        <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            National Scholarship for PhD Students, 2018.</li>
        </ul>
        <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            National English Competition, Third Prize, 2013.</li>
        </ul>
       <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            National Mathematical Modeling Competition, Second Prize, 2012.</li>
        </ul>
      </div>
        
        
        </br> 
        <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
            <a href="https://scholar.google.com/citations?user=l0z2QNQAAAAJ&hl=en" target="_blank">
              <i class="ai ai-google-scholar-square ai-3x"></i>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="https://github.com/YangLiu9208" target="_blank">
              <i class="fa fa-github-square fa-3x"></i>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="https://dblp.uni-trier.de/pid/51/3710-84.html" target="_blank">
              <i class="ai ai-dblp-square ai-3x"></i>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="https://www.researchgate.net/profile/Yang-Liu-30" target="_blank">
              <i class="ai ai-researchgate-square ai-3x"></i>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="https://orcid.org/0000-0002-9423-9252" target="_blank">
              <i class="ai ai-orcid-square ai-3x"></i>
            </a>
          </li>
          </ul>
        <br>
        <div class="mb-5"> 
            <b>Related Link</b>: <a href="https://ccfddl.github.io/" target="_blank">AI Conference Calendar</a>, <a href="https://www.ccf.org.cn/c/2016-12-27/569124.shtml" target="_blank">CCF Rankings</a>
        </div>    
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=m&d=Ng_LP0UrQP5gWGJEym9UVNi8Up0eZ-cQXq9wH1Me7Bg"></script>
      </div>
    </section>
    
    <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="News">
      <div class="my-auto">
        <h3 class="mb-5">News</h3>
          <ul>
              <li>2024-04:  The book of multimodal large model (<a href= "https://hcplab-sysu.github.io/Book-of-MLM/" target="_blank">《多模态大模型：新一代人工智能技术范式》</a>) is published!
              <li>2023-12:  I won the third prize of CCF ChinaSoft 2023 Robotic Big Model and Embodied Intelligence Challenge!
              <li>2023-11:  One first-author T-IP paper is selected as the ESI Hot Cited Paper!
              <li>2023-11:  One first-author T-IP paper is selected as the ESI Highly Cited Paper!
              <li>2023-10:  One accepted ACM MM 2023 paper is recommended as an oral!
              <li>2023-10:  An invention patent has been granted.
              <li>2023-10:  I have been selected as a member of the CSIG Visual Big Data Committee.
              <li>2023-07:  One paper is accepted by ACM MM 2023! 
              <li>2023-07:  Two papers are accepted by ICCV 2023! 
              <li>2023-06:  One paper is accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence! 
              <li>2023-06:  I won the first prize in the Third Guangdong Province Young Computer Science Academic Show. 
              <li>2023-08:  One paper is accepted by PRCV 2023.
              <li>2023-06:  One paper is accepted by ICIP 2023. 
              <li>2023-04:  One paper is accepted by IJCAI 2023! 
              <li>2023-03:  The open-source framework for causal discovery and visual-linguistic reasoning <a href= "https://github.com/YangLiu9208/Causal-VLReasoning" target="_blank">Causal-VLReasoning</a> is online!
              <li>2023-03:  I become a member of CAAI (Chinese Association for Artificial Intelligence).
              <li>2023-03:  I become a member of CCF (China Computer Federation).
              <li>2023-03:  I am invited as the Program Committee (PC) Member for the <a href="http://www.cgs-network.org/cgi23/" target="_blank">Computer Graphics International (CGI) 2023</a>. 
              <li>2023-03:  One paper is accepted by Information Sciences. 
              <li>2022-12:  One <a href= "https://link.springer.com/article/10.1007/s11633-022-1362-z" target="_blank">review paper</a> about causal reasoning is reported in <a href="https://mp.weixin.qq.com/s/-OlJ44DWE6nuX_OVyykURw" target="_blank">social media</a> by Machine Intelligence Research.
              <li>2022-11:  I received the funding from Guangdong Basic and Applied Basic Research Foundation 2023.
              <li>2022-11:  One <a href="https://link.springer.com/article/10.1007/s11633-022-1362-z" target="_blank">review paper</a> for causal reasoning is published online by Machine Intelligence Research.  
              <li>2022-11:  I am invited as the Program Committee (PC) Member for the <a href="http://iccvm.org/2023/" target="_blank">Computational Visual Media Conference 2023</a>. 
              <li>2022-11:  I am invited as the reviewer of IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023.
              <li>2022-09:  I become a member of IEEE Industrial Electronics Society.
              <li>2022-08:  My <a href="https://github.com/YangLiu9208/TCGL" target="_blank">TCGL</a> paper published in IEEE T-IP is invited to be exhibited as the poster in <a href="http://valser.org/2022/#/poster" target="_blank">Valse 2022</a>.
              <li>2022-08:  One review paper about causal reasoning is accepted by Machine Intelligence Research. 
              <li>2022-08:  I am invited as the Program Committee (PC) Member for the AAAI 2023. 
              <li>2022-06:  I become a member of CSIG.
              <li>2022-05:  One paper is accepted by IEEE Transactions on Industrial Informatics. 
              <li>2022-04:  I received the funding from Science and Technology Projects in Guangzhou.
              <li>2022-03:  One paper is accepted by CVPR 2022 as an oral presentation. 
              <li>2022-01:  One paper is accepted by IEEE Transactions on Image Processing. <a href="v" target="_blank">Code</a> is available.
              <li>2022-01:  One paper is accepted by ICASSP 2022. 
              <li>2022-01:  The code for <a href="https://github.com/YangLiu9208/TCGL" target="_blank">TCGL</a> is available.
              <li>2021-12:  I am invited as the reviewer of Chinese Conference on Pattern Recognition and Computer Vision (PRCV) 2022.
              <li>2021-11:  I am invited as the reviewer of IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022.
              <li>2021-10:  I start working as a research associate professor at Sun-Yat-Sen University.
              <li>2021-05:  One paper is accepted by IEEE Transactions on Image Processing. <a href="https://yangliu9208.github.io/SAKDN/" target="_blank">Code</a> is available.
              <li>2021-04:  The code for SAKDN is available.
              <li>2021-02:  I become a Member of the IEEE.
              <li>2020-12:  I am invitied as the reviewing expert for NSFC.
              <li>2020-11:  I received the funding from National Natural Science Foundation of Guangdong Province of China 2021.
              <li>2020-09:  I received the funding from National Natural Science Foundation of China 2021.
              <li>2020-06:  I received the funding from China Postdoctoral Science Foundation 2020.
              <li>2020-03:  I received the funding from Fundamental Research of the Central Universities 2020.
              <li>2020-01:  My Ph.D Thesis (psw：ithd) is available now.
              <li>2019-10:  Codes and datasets for IEEE TIP is available.
              <li>2019-10:  One paper is accepted by IEEE Transactions on Image Processing.
              <li>2019-08:  I start working as a postdoctoral fellow at Sun-Yat-Sen University.
              <li>2019-06:  I obtain my Ph.D degree from Xidian University.
          </ul>
      </div>
    </section>    
    
    <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div>    
         <h3 class="mb-5">Open-source Framework</h3>   
            <ol style="list-style-type:none">
                           <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                                <li><img class="product-cell-img" src="img/CausalVLR.gif"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal Reasoning</i></b></br> 
               <b>Yang Liu</b>, Weixing Chen, Guanbin Li, Liang Lin
                 <table class="imgtable">
                 <tr>
                    <td>
                         <p class="pub_link">[<a href="https://github.com/HCPLab-SYSU/CausalVLR" target="_blank">CausalVLR</a>] 
                                                [<a href= "https://arxiv.org/pdf/2306.17462.pdf" target="_blank">Paper</a>] 
                                                [<a  onclick="display('Causal')" style="cursor:pointer;">BibTex</a>]
                             </p>
                     </td>
                 </tr>
                 </table>
               <div>
                        <pre id="Causal" style="display: none;">
                            @article{CausalVLR,
                              title={CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal Reasoning},
                              author={Liu, Yang and Chen, Weixing and Li, Guanbin and Lin, Liang},
                              journal={arXiv preprint arXiv:2306.17462},
                              year={2023}
                            }
                            }
                          </pre>
                         </div>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/CausalVLR"></a>
                <font color="#0000FF ">Research topic: Causal Reasoning,  Visual-Linguistic Reasoning, Open-source Framework and Benchmark</font>
             </div>
           </div>  
                   </ol>  
      </div>
   
                <div>  
                 <h3 class="mb-5">Demos of Embodied AI</h3>   
                    <ol style="list-style-type:none">
                                     <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                    <li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/Demo_Public_0421.mp4"> <source src="img/Demo_Public_0421.mp4" type="video/mp4"></video></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Embodied Interactive Robot</i></b></br> 
             </div>
           </div>  
                           <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                    <li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/EmbodiedAgent_low.mp4"> <source src="img/EmbodiedAgent_low.mp4" type="video/mp4"></video></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Embodied Interactive Agent for the Coffee Scene</i></b></br> 
             </div>
           </div>  
      </div>
         </ol>
        <div>  
                 <h3 class="mb-5">Book</h3>   
                                  <ol style="list-style-type:none">
                           <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                                <li><img class="product-cell-img" src="img/PHE.jpg" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Multimodal Large Models: A New Generation of Artificial Intelligence Technology Paradigm</i></b></br> 
                  <b><i>多模态大模型：新一代人工智能技术范式</i></b></br> 
                 <b>Publishing House of Electronics Industry (PHE), ISBN 978-7-121-47547-4, 2024.</b></br>
                <b>电子工业出版社，ISBN 978-7-121-47547-4, 2024.</b></br>
               <b>Yang Liu</b>, Liang Lin</br>
                             <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://hcplab-sysu.github.io/Book-of-MLM/" target="_blank">Resource Page (在线资源)</a>] [<a href= "https://item.jd.com/10100489294930.html" target="_blank">JD Page (京东商城链接)</a>][<a href= "https://mp.weixin.qq.com/s/WHYy-dlJl6V4TQoZxWIiYQ" target="_blank">媒体报道</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                             <a href="https://github.com/HCPLab-SYSU/Book-of-MLM/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/Book-of-MLM"></a>
      <a href="https://github.com/HCPLab-SYSU/Book-of-MLM/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/Book-of-MLM"></a>
      <a href=https://github.com/HCPLab-SYSU/Book-of-MLM/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/Book-of-MLM"></a>
             </div>
           </div>  
      </div>
 </ol>
        
        <h3 class="mb-5">Selected Papers</h3>   
                    <ol style="list-style-type:none">
        
                            <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.gif"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/VLCIR.gif" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering</i></b></br>
               <b>Yang Liu</b>, Guanbin Li, Liang Lin<br>
                <b>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023.</b> 
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/10146482" target="_blank">Paper</a>] 
                         [<a href= "https://mp.weixin.qq.com/s/RRVIACXRLA0-nePQO5bY6g" target="_blank">中文解读</a>]    
                         [<a href="https://github.com/HCPLab-SYSU/CMCIR" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('CMCIR')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="CMCIR" style="display: none;">
                                  @article{liu2022cross,
                                         author={Liu, Yang and Li, Guanbin and Lin, Liang},
                                          journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
                                          title={Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering}, 
                                          year={2023},
                                          volume={},
                                          number={},
                                          pages={1-17},
                                          doi={10.1109/TPAMI.2023.3284038}
                                            }
                          </pre>
                         </div>
      <a href="https://github.com/HCPLab-SYSU/CMCIR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/CMCIR"></a>
      <a href="https://github.com/HCPLab-SYSU/CMCIR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/CMCIR"></a>
      <a href="https://github.com/HCPLab-SYSU/CMCIR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/CMCIR"></a>
      <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:LhH-TYMQEocC" target="_blank"></a>
    
               <font color="#0000FF ">Research topic: Causal Reasoning, Video Question Answering</font>
           </div> 
           </div> 

                                                        <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/MEIA.jpg"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/MEIA.jpg" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>MEIA: Towards Realistic Multimodal Interaction and Manipulation for Embodied Robots</i></b></br>
              <b>Yang Liu</b>, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin<br>
                 <table class="imgtable">
                 <tr>
                    <td> 
                         <p class="pub_link">[<a href= "https://arxiv.org/pdf/2402.00290.pdf" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('MEIA')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="MEIA" style="display: none;">
                          </pre>
                         </div>
               <font color="#0000FF ">Research topic: Embodied AI, Agent, Robotics</font>
           </div> 
           </div> 

                                                        <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.gif"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/CaCoCOT.gif" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>CausalGPT: Illuminating Faithfulness and Causality for Knowledge-based Reasoning with LLMs</i></b></br>
              Ziyi Tang, Ruilin Wang, Weixing Chen, Yongsen Zheng, <b>Yang Liu</b>, Keze Wang, Tianshui Chen, Liang Lin<br>
                 <table class="imgtable">
                 <tr>
                    <td> 
                        <p class="pub_link">[<a href= "https://arxiv.org/pdf/2308.11914.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/HCPLab-SYSU/CausalVLR" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('CausalGPT')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="CausalGPT" style="display: none;">
                                @article{tang2023towards,
                                  title={Towards causalgpt: A multi-agent approach for faithful knowledge reasoning via promoting causal consistency in llms},
                                  author={Tang, Ziyi and Wang, Ruilin and Chen, Weixing and Wang, Keze and Liu, Yang and Chen, Tianshui and Lin, Liang},
                                  journal={arXiv preprint arXiv:2308.11914},
                                  year={2023}
                                }
                          </pre>
                         </div>
      <a href=https://github.com/HCPLab-SYSU/CausalVLR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:S16KYo8Pm5AC" target="_blank"></a>
               <font color="#0000FF ">Research topic: Causal Reasoning, Chain-of-Thought</font>
           </div> 
           </div> 
              
                                          <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.gif"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/VLCI.gif" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Cross-Modal Causal Intervention for Medical Report Generation</i></b></br>
               Weixing Chen, <b>Yang Liu</b><sup>✉</sup>, Ce Wang, Jiarui Zhu, Shen Zhao, Guanbin Li, Cheng-Lin Liu, Liang Lin<br>
                 <table class="imgtable">
                 <tr>
                    <td> 
                         [<a href="https://github.com/WissingChen/VLCI" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('VLCI')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="VLCI" style="display: none;">
                                @article{chen2023visual,
                                  title={Visual-linguistic causal intervention for radiology report generation},
                                  author={Chen, Weixing and Liu, Yang and Wang, Ce and Li, Guanbin and Zhu, Jiarui and Lin, Liang},
                                  journal={arXiv preprint arXiv:2303.09117},
                                  year={2023}
                                }
                          </pre>
                         </div>
      <a href=https://github.com/WissingChen/VLCI/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/WissingChen/VLCI"></a>
      <a href="https://github.com/WissingChen/VLCI/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/WissingChen/VLCI"></a>
      <a href="https://github.com/WissingChen/VLCI/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/WissingChen/VLCI"></a>
      <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:J-pR_7NvFogC" target="_blank"></a>
               <font color="#0000FF ">Research topic: Causal Reasoning, Medical Report Generation</font>
           </div> 
           </div> 
                                                     <!-- 文章序列10 -->
          
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/VCSR.gif" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Visual Causal Scene Refinement for Video Question Answering</i></b></br> 
               Yushen Wei*, <b>Yang Liu*</b>, Hong Yan, Guanbin Li, Liang Lin<sup>✉</sup><br>
                                              <b>ACM International Conference on Multimedia (ACM MM), 2023. (* indicates co-first author)</b> (<font color="#FF0000">Oral</font>)
                                                              <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://arxiv.org/pdf/2305.04224.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/HCPLab-SYSU/CausalVLR" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('VCSR')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="VCSR" style="display: none;">
                            @inproceedings{10.1145/3581783.3611873,
                            author = {Wei, Yushen and Liu, Yang and Yan, Hong and Li, Guanbin and Lin, Liang},
                            title = {Visual Causal Scene Refinement for Video Question Answering},
                            year = {2023},
                            isbn = {9798400701085},
                            publisher = {Association for Computing Machinery},
                            address = {New York, NY, USA},
                            url = {https://doi.org/10.1145/3581783.3611873},
                            doi = {10.1145/3581783.3611873},
                            pages = {377–386},
                            numpages = {10},
                            keywords = {causal reasoning, video question answering, cross-modal},
                            location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
                            series = {MM '23}
                            }
                          </pre>
                         </div>
    <a href="https://github.com/HCPLab-SYSU/CausalVLR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/CausalVLR"></a>
    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:mlAyqtXpCwEC" target="_blank"></a>
                <font color="#0000FF ">Research topic:  Causal Discovery, Cross-Modal Question Reasoning</font>
             </div>
           </div>
   
                                                   <!-- 文章序列10 -->
          
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/SkeletonMAE.png" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training</i></b></br> 
               Hong Yan, <b>Yang Liu</b><sup>✉</sup>, Yushen Wei, Zhen Li, Guanbin Li, Liang Lin<br>
                                              <b>IEEE/CVF International Conference on Computer Vision (ICCV), 2023.</b> 
                                                              <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_SkeletonMAE_Graph-based_Masked_Autoencoder_for_Skeleton_Sequence_Pre-training_ICCV_2023_paper.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/HongYan1123/SkeletonMAE" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('SkeletonMAE')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="SkeletonMAE" style="display: none;">
                            @inproceedings{yan2023skeletonmae,
                              title={Skeletonmae: graph-based masked autoencoder for skeleton sequence pre-training},
                              author={Yan, Hong and Liu, Yang and Wei, Yushen and Li, Zhen and Li, Guanbin and Lin, Liang},
                              booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
                              pages={5606--5618},
                              year={2023}
                            }
                          </pre>
                         </div>
                       <a href="https://github.com/HongYan1123/SkeletonMAE/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HongYan1123/SkeletonMAE"></a>
      <a href="https://github.com/HongYan1123/SkeletonMAE/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HongYan1123/SkeletonMAE"></a>
      <a href="https://github.com/HongYan1123/SkeletonMAE/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HongYan1123/SkeletonMAE"></a>
    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:_FM0Bhl9EiAC" target="_blank"></a>
                <font color="#0000FF ">Research topic:  Self-supervised Learning, Skeleton Action Recognition</font>
             </div>
           </div>  

                                                                 <!-- 文章序列10 -->
          
          <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/ESL.png" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Enhanced Soft Label for Semi-Supervised Semantic Segmentation</i></b></br> 
               Jie Ma, Chuan Wang, <b>Yang Liu</b>, Liang Lin, Guanbin Li<br>
                                              <b>IEEE/CVF International Conference on Computer Vision (ICCV), 2023.</b> 
                                                              <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Enhanced_Soft_Label_for_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/unrealMJ/ESL" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('ESL')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="ESL" style="display: none;">
                                @inproceedings{ma2023enhanced,
                                  title={Enhanced Soft Label for Semi-Supervised Semantic Segmentation},
                                  author={Ma, Jie and Wang, Chuan and Liu, Yang and Lin, Liang and Li, Guanbin},
                                  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
                                  pages={1185--1195},
                                  year={2023}
                                }
                          </pre>
                         </div>
                       <a href="https://github.com/unrealMJ/ESL/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/unrealMJ/ESL"></a>
      <a href="https://github.com/unrealMJ/ESL/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/unrealMJ/ESL"></a>
      <a href="https://github.com/unrealMJ/ESL/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/unrealMJ/ESL"></a>
        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:FAceZFleit8C" target="_blank"></a>
                <font color="#0000FF ">Research topic: Semantic Segmentationg, Semi-Supervised Learning</font>
             </div>
           </div>
              
                                                       <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                 <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.mp4"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
                <li><img class="product-cell-img" src="img/DenseLight.png" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>DenseLight: Efficient Control for Large-scale Traffic Signals with Dense Feedback</i></b></br>
               Junfan Lin, Yuying Zhu, Lingbo Liu, <b>Yang Liu</b><sup>✉</sup>, Guanbin Li, Liang Lin<br>
                <b>International Joint Conference on Artificial Intelligence (IJCAI), 2023.</b> 
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://yangliu9208.github.io/home/" target="_blank">Paper</a>] 
                         [<a href=" https://github.com/junfanlin/DenseLight" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('DenseLight')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="DenseLight" style="display: none;">
                            @inproceedings{ijcai2023p672,
                              title     = {DenseLight: Efficient Control for Large-scale Traffic Signals with Dense Feedback},
                              author    = {Lin, Junfan and Zhu, Yuying and Liu, Lingbo and Liu, Yang and Li, Guanbin and Lin, Liang},
                              booktitle = {Proceedings of the Thirty-Second International Joint Conference on
                                           Artificial Intelligence, {IJCAI-23}},
                              publisher = {International Joint Conferences on Artificial Intelligence Organization},
                              editor    = {Edith Elkind},
                              pages     = {6058--6066},
                              year      = {2023},
                              month     = {8},
                              note      = {AI for Good},
                              doi       = {10.24963/ijcai.2023/672},
                              url       = {https://doi.org/10.24963/ijcai.2023/672},
                            }
                                                      </pre>
                         </div>
                     <a href="https://github.com/junfanlin/DenseLight/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/junfanlin/DenseLight"></a>
      <a href="https://github.com/junfanlin/DenseLight/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/junfanlin/DenseLight"></a>
      <a href="https://github.com/junfanlin/DenseLight/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/junfanlin/DenseLight"></a>
    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:olpn-zPbct0C" target="_blank"></a>
               <font color="#0000FF ">Research topic: Traffic Signal Control, Reinforcement Learning from Human Feedback (RLHF)</font>
           </div> 
           </div> 
              
                                                                   <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/IS.gif"> <source src="img/IS.mp4" type="video/mp4"></video></li>--> 
              <li><img class="product-cell-img" src="img/IS.gif" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Urban Regional Function Guided Traffic Flow Prediction</i></b> </br> 
               Kuo Wang, Lingbo Liu, <b>Yang Liu</b><sup>✉</sup>, Guanbin Li, Liang Lin<br>
                <b>Information Sciences (INS), 2023.</b> 
                                               <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://www.sciencedirect.com/science/article/pii/S0020025523004334" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('TFP')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="TFP" style="display: none;">
                                  @article{TFP,
                                        title = {Urban regional function guided traffic flow prediction},
                                        journal = {Information Sciences},
                                        volume = {634},
                                        pages = {308-320},
                                        year = {2023},
                                        issn = {0020-0255},
                                        doi = {https://doi.org/10.1016/j.ins.2023.03.109},
                                        url = {https://www.sciencedirect.com/science/article/pii/S0020025523004334},
                                        author = {Kuo Wang and LingBo Liu and Yang Liu and GuanBin Li and Fan Zhou and Liang Lin},
                                            }
                          </pre>
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:OP4eGU-M3BUC" target="_blank"></a>
                <font color="#0000FF ">Research topic: Traffic Flow Prediction, Spatial-temporal Representation Learning</font>
                                                     </div>
             </div>
           </div> 

                                                                               <!-- 文章序列10 -->
          
          <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/DADA.png" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Dual adversarial adaptation for cross-device real-world image super-resolution</i></b></br> 
               Xiaoqian Xu, Pengxu Wei, Weikai Chen, <b>Yang Liu</b>, Mingzhi Mao, Liang Lin, Guanbin Li<br>
                                              <b>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</b> (<font color="#FF0000">Oral</font>)
                                                              <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Dual_Adversarial_Adaptation_for_Cross-Device_Real-World_Image_Super-Resolution_CVPR_2022_paper.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/lonelyhope/DADA" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('DADA')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="DADA" style="display: none;">
                            @inproceedings{xu2022dual,
                              title={Dual adversarial adaptation for cross-device real-world image super-resolution},
                              author={Xu, Xiaoqian and Wei, Pengxu and Chen, Weikai and Liu, Yang and Mao, Mingzhi and Lin, Liang and Li, Guanbin},
                              booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                              pages={5667--5676},
                              year={2022}
                            }
                          </pre>
                         </div>
                       <a href="https://github.com/lonelyhope/DADA/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/lonelyhope/DADA"></a>
      <a href="https://github.com/lonelyhope/DADA/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/lonelyhope/DADA"></a>
      <a href=https://github.com/lonelyhope/DADA/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/lonelyhope/DADA"></a>
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:VaXvl8Fpj5cC" target="_blank"></a>
                <font color="#0000FF ">Research topic: Image Super-resolution</font>
             </div>
           </div>
              
                    <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                 <li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/MIR.mp4"> <source src="img/MIR.mp4" type="video/mp4"></video></li> 
                <!-- <li><img class="product-cell-img" src="img/CSTL.png"></li>-->
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Causal Reasoning Meets Visual Representation Learning: A Prospective Study</i></b></br>   
                <b>Yang Liu</b>, Yushen Wei, Hong Yan, Guanbin Li, Liang Lin<br>
                <b>Machine Intelligence Research (MIR), 2022.</b> (<font color="#FF0000">Top-10 Downloads</font>)
                                <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://link.springer.com/article/10.1007/s11633-022-1362-z" target="_blank">Paper</a>] 
                         [<a href= " https://arxiv.org/abs/2204.12037" target="_blank">Arxiv (Keep Updating)</a>]   
                         [<a href="https://youtu.be/2lfNaTkcTHI" target="_blank">Video (Youtube)</a>] 
                         [<a href="https://www.bilibili.com/video/BV1gP411o7m2" target="_blank">Video (BiliBili)</a>] 
                         [<a href="https://mp.weixin.qq.com/s/-OlJ44DWE6nuX_OVyykURw" target="_blank">中文解读</a>] 
                        [<a  onclick="display('Review')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="Review" style="display: none;">
                                @article{liu2022causal,
                                  title={Causal Reasoning Meets Visual Representation Learning: A Prospective Study},
                                  author={Liu, Yang and Wei, Yu-Shen and Yan, Hong and Li, Guan-Bin and Lin, Liang},
                                  journal={Machine Intelligence Research},
                                  pages={1--27},
                                  year={2022},
                                  publisher={Springer}
                                }
                          </pre>
                         </div>
                    <a href="https://github.com/YangLiu9208/MIR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/MIR"></a>
      <a href="https://github.com/YangLiu9208/MIR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/MIR"></a>
      <a href="https://github.com/YangLiu9208/MIR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/MIR"></a>
                        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:a3BOlSfXSfwC" target="_blank"></a>
                <font color="#0000FF ">Research topic: Causal Reasoning, Spatial-temporal Representation Learning</font>
             </div>
           </div> 

                       <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                 <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/TII.gif"> <source src="img/TII.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/TII.gif" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Hybrid-Order Representation Learning for Electricity Theft Detection</i></b></br>  
                Yuying Zhu, Yang Zhang, Lingbo Liu, <b>Yang Liu</b><sup>✉</sup>, Guanbin Li, Mingzhi Mao, Liang Lin<br>
                <b>IEEE Transactions on Industrial Informatics (T-II), 2022.</b>
                                <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/9785914" target="_blank">Paper</a>] 
                         [<a href="https://github.com/GillianZhu/HORLN" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('HORLN')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="HORLN" style="display: none;">
                    @article{zhu2022hybrid,
                      title={Hybrid-Order Representation Learning for Electricity Theft Detection},
                      author={Zhu, Yuying and Zhang, Yang and Liu, Lingbo and Liu, Yang and Li, Guanbin and Mao, Mingzhi and Lin, Liang},
                      journal={IEEE Transactions on Industrial Informatics},
                      volume={19},
                      number={2},
                      pages={1248--1259},
                      year={2023},
                      publisher={IEEE}
                    }
                          </pre>
                         </div>
      <a href="https://github.com/GillianZhu/HORLN/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/GillianZhu/HORLN"></a>
      <a href="https://github.com/GillianZhu/HORLN/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/GillianZhu/HORLN"></a>
      <a href="https://github.com/GillianZhu/HORLN/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/GillianZhu/HORLN"></a>
                            <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:F9fV5C73w3QC" target="_blank"></a>
                 <font color="#0000FF ">Research topic: Spatial-temporal Anomaly Detection</font>
             </div>
           </div> 

                                                                                               <!-- 文章序列10 -->
          <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/IS.gif"> <source src="img/IS.mp4" type="video/mp4"></video></li>--> 
              <li><img class="product-cell-img" src="img/VSAR.png" style="width=200"></li>
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>Cross-modal knowledge distillation for Vision-to-Sensor action recognition</i></b> </br> 
              Jianyuan Ni, Raunak Sarbajna, <b>Yang Liu</b>, Anne HH Ngu, Yan Yan<br>
                <b>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022.</b> 
                                               <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/abstract/document/9746752" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('VSAR')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="TFP" style="display: none;">
                                  @inproceedings{ni2022cross,
                              title={Cross-modal knowledge distillation for Vision-to-Sensor action recognition},
                              author={Ni, Jianyuan and Sarbajna, Raunak and Liu, Yang and Ngu, Anne HH and Yan, Yan},
                              booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
                              pages={4448--4452},
                              year={2022},
                              organization={IEEE}
                            }
                          </pre>
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:Ug5p-4gJ2f0C" target="_blank"></a>
                <font color="#0000FF ">Research topic: Multi-modal Spatial-temporal Knowledge Transfer</font>
                                                     </div>
             </div>
           </div>
                                   <!-- 文章序列9 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                 <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/TCGL.gif"> <source src="img/TCGL.mp4" type="video/mp4"></video></li>--> 
                 <li><img class="product-cell-img" src="img/TCGL.gif" style="width=200"></li> 
             </div>
             <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
               <b><i>TCGL: Temporal Contrastive Graph for Self-supervised Video Representation Learning</i></b></br>  
                <b>Yang Liu</b>, Keze Wang, Lingbo Liu, Haoyuan Lan, Liang Lin<br>
                <b>IEEE Transactions on Image Processing</b> (<b>T-IP</b>), <b>2022.</b> (<font color="#FF0000"><b>ESI Highly Cited Paper</b></font>)
                                <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/9713748" target="_blank">Paper</a>] 
                         [<a href="https://github.com/YangLiu9208/TCGL/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('TCGL')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="TCGL" style="display: none;">
                            @article{liu2022tcgl,
                              title={TCGL: Temporal Contrastive Graph for Self-Supervised Video Representation Learning},
                              author={Liu, Yang and Wang, Keze and Liu, Lingbo and Lan, Haoyuan and Lin, Liang},
                              journal={IEEE Transactions on Image Processing},
                              volume={31},
                              pages={1978--1993},
                              year={2022},
                              publisher={IEEE}
                            }
                          </pre>
                         </div>
      <a href="https://github.com/YangLiu9208/TCGL/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/TCGL"></a>
      <a href="https://github.com/YangLiu9208/TCGL/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/TCGL"></a>
      <a href="https://github.com/YangLiu9208/TCGL/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/TCGL"></a>
                            <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:P7Ujq4OLJYoC" target="_blank"></a>
                <font color="#0000FF ">Research topic: Self-supervised Spatial-temporal Representation Learning</font>
             </div>
           </div> 

                     <!-- 文章序列7 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                  <!-- <li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/SAKDN.gif"> <source src="img/SAKDN.mp4" type="video/mp4"></video></li> --> 
                <li><img class="product-cell-img" src="img/SAKDN.gif" style="width=200"></li>
              </div>
              <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
                <b><i>Semantics-aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition</i></b></br>  
                  <b>Yang Liu</b>, Keze Wang, Guanbin Li, Liang Lin<br>
                  <b>IEEE Transactions on Image Processing</b> (<b>T-IP</b>), <b>2021</b>.
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/9451581" target="_blank">Paper</a>] 
                         [<a href="https://github.com/YangLiu9208/SAKDN" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('SAKDN')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="SAKDN" style="display: none;">
                            @article{liu2021semantics,
                              title={Semantics-aware adaptive knowledge distillation for sensor-to-vision action recognition},
                              author={Liu, Yang and Wang, Keze and Li, Guanbin and Lin, Liang},
                              journal={IEEE Transactions on Image Processing},
                              volume={30},
                              pages={5573--5588},
                              year={2021},
                              publisher={IEEE}
                            }
                          </pre>
                         </div>
      <a href="https://github.com/YangLiu9208/SAKDN/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/SAKDN"></a>
      <a href="https://github.com/YangLiu9208/SAKDN/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/SAKDN"></a>
      <a href="https://github.com/YangLiu9208/SAKDN/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/SAKDN"></a>
 <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:KbBQZpvPDL4C" target="_blank"></a>
                  <font color="#0000FF ">Research topic: Multi-modal Spatial-temporal Knowledge Transfer</font>
              </div>
            </div>

                 <!-- 文章序列6 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                   <!-- <li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/DIVAFN.gif"> <source src="img/DIVAFN.mp4" type="video/mp4"></video></li>--> 
                <li><img class="product-cell-img" src="img/DIVAFN.gif" style="width=200"></li> 
              </div>
              <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
                <b><i>Deep Image-to-Video Adaptation and Fusion Networks for Action Recognition</i></b></br>  
                  <b>Yang Liu</b>, Zhaoyang Lu, Jing Li, Tao Yang, Chao Yao<br>
                  <b>IEEE Transactions on Image Processing</b> (<b>T-IP</b>), <b>2020</b>. 
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/8931264" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/DIVAFN/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('DIVAFN')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="DIVAFN" style="display: none;">
                            @article{liu2019deep,
                              title={Deep image-to-video adaptation and fusion networks for action recognition},
                              author={Liu, Yang and Lu, Zhaoyang and Li, Jing and Yang, Tao and Yao, Chao},
                              journal={IEEE Transactions on Image Processing},
                              volume={29},
                              pages={3168--3182},
                              year={2019},
                              publisher={IEEE}
                            }
                          </pre>
                         </div>
      <a href="https://github.com/YangLiu9208/DIVAFN/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/DIVAFN"></a>
      <a href="https://github.com/YangLiu9208/DIVAFN/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/DIVAFN"></a>
      <a href="https://github.com/YangLiu9208/DIVAFN/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/DIVAFN"></a>
<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&cstart=20&pagesize=80&citation_for_view=l0z2QNQAAAAJ:r_AWSJRzSzQC" target="_blank"></a>
                 <font color="#0000FF ">Research topic: Multi-modal Spatial-temporal Knowledge Transfer</font>
              </div>
            </div> 


            <!-- 文章序列5 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                                     <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/TCSVT.mp4"> <source src="img/TCSVT.mp4" type="video/mp4"></video></li>-->
                 <li><img class="product-cell-img" src="img/TCSVT.gif" style="width=200"></li>
              </div>
              <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
                <b><i>Hierarchically Learned View-Invariant Representations for Cross View Action Recognition</i></b></br> 
                  <b>Yang Liu</b>, Zhaoyang Lu, Jing Li, Tao Yang<br>
                  <b>IEEE Transactions on Circuits and Systems for Video Technology</b> (<b>T-CSVT</b>), <b>2019</b>.
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/8453034" target="_blank">Paper</a>] 
                         [<a href="https://github.com/YangLiu9208/JSRDA/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('JSRDA')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="JSRDA" style="display: none;">
                            @article{liu2018hierarchically,
                              title={Hierarchically learned view-invariant representations for cross-view action recognition},
                              author={Liu, Yang and Lu, Zhaoyang and Li, Jing and Yang, Tao},
                              journal={IEEE Transactions on Circuits and Systems for Video Technology},
                              volume={29},
                              number={8},
                              pages={2416--2430},
                              year={2018},
                              publisher={IEEE}
                            }
                          </pre>
                         </div>
      <a href="https://github.com/YangLiu9208/JSRDA/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/JSRDA"></a>
      <a href="https://github.com/YangLiu9208/JSRDA/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/JSRDA"></a>
      <a href="https://github.com/YangLiu9208/JSRDA/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/JSRDA"></a>  
<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&cstart=20&pagesize=80&citation_for_view=l0z2QNQAAAAJ:HeT0ZceujKMC" target="_blank"></a>
                <font color="#0000FF ">Research topic: Multi-view Spatial-temporal Knowledge Transfer</font>
              </div>
            </div>      
              
            <!-- 文章序列4 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                   <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/SPL.gif"> <source src="img/SPL.mp4" type="video/mp4"></video></li> -->
               <li><img class="product-cell-img" src="img/SPL.gif" style="width=200"></li>
              </div>
              <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
                <b><i>Global Temporal Representation based CNNs for Infrared Action Recognition</i></b></br>  
                  <b>Yang Liu</b>, Zhaoyang Lu, Jing Li, Tao Yang, Chao Yao<br>
                  <b>IEEE Signal Processing Letters</b> (<b>SPL</b>), <b>2018</b>.
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/8332532" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/TSTDDs/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('TSTDDs')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="TSTDDs" style="display: none;">
                            @article{liu2018global,
                              title={Global temporal representation based cnns for infrared action recognition},
                              author={Liu, Yang and Lu, Zhaoyang and Li, Jing and Yang, Tao and Yao, Chao},
                              journal={IEEE Signal Processing Letters},
                              volume={25},
                              number={6},
                              pages={848--852},
                              year={2018},
                              publisher={IEEE}
                            }
                          </pre>
                         </div>
      <a href="https://github.com/YangLiu9208/TSTDDs/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/TSTDDs"></a>
      <a href="https://github.com/YangLiu9208/TSTDDs/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/TSTDDs"></a>
      <a href="https://github.com/YangLiu9208/TSTDDs/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/TSTDDs"></a>  
<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&cstart=20&pagesize=80&citation_for_view=l0z2QNQAAAAJ:yMeIxYmEMEAC" target="_blank"></a>
                  <font color="#0000FF ">Research topic: Heterogenous Spatial-temporal Representation Learning</font>
              </div>
            </div>              
       </ol>
            
<h3 class="mb-5">PhD Dissertation</h3>   
             <ol style="list-style-type:none">
                           <!-- 文章序列0 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-2 col-md-3 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/XD.gif"> <source src="img/XD.mp4" type="video/mp4"></video></li>--> 
                <li><img class="product-cell-img" src="img/XD.gif"></li>
              </div>
              <div class="product-cell-text col-lg-10 col-md-9 col-sm-12 col-xs-12">
                <b><i>Cross-domain Human Action Recognition via Transfer Learning (基于迁移学习的跨域人体行为识别研究)</i></b> 
                  <br><b>PhD Dissertation (博士学位论文), Xidian University (西安电子科技大学), June 30, 2019.</b></br> 
                  <b>Yang Liu</b><br>
                  <b>Supervisor：Prof. Zhaoyang Lu</b><br>
                
                                                                               <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://cdmd.cnki.com.cn/Article/CDMD-10701-1020000550.htm" target="_blank">Paper</a>] 
                        [<a  onclick="display('PHD')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="PHD" style="display: none;">
                        @phdthesis{刘阳2019基于迁移学习的跨域人体行为识别研究,
                          title={基于迁移学习的跨域人体行为识别研究},
                          author={刘阳},
                          year={2019},
                          school={西安电子科技大学}
                        }
                          </pre>
                         </div>
              </div>
            </div>
 </ol>
             
          
    </section>
    
 
    
    <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="activities">
      <div class="my-auto">
        <h3 class="mb-5">Academic services</h3>
          <ul>
              <b>Reviewer for Journals</b>
              <li>Advanced Science
              <li>IEEE Transactions on Pattern Analysis and Machine Intelligence
              <li>IEEE Transactions on Image Processing
              <li>IEEE Transactions on Neural Networks and Learning Systems
              <li>IEEE Transactions on Cybernetics   
              <li>IEEE Transactions on Multimedia
              <li>IEEE Transactions on Circuits and Systems for Video Technology
              <li>IEEE Transactions on Human-Machine Systems
              <li>IEEE Signal Processing Letters
              <li>IEEE Robotics and Automation Letters
              <li>IET Computer Vision
              <li>Pattern Recognition
              <li>Pattern Recognition Letters
              <li>Machine Vision and Applications    
              <li>Journal of Visual Communication and Image Representation    
              <li>Visual Computer
          </ul>
          <ul>
              <b>Area Chair (AC)/Program Committee (PC)/Reviewer for Conferences</b>
              <li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
              <li>IEEE/CVF International Conference on Computer Vision (ICCV)
              <li>European Conference on Computer Vision (ECCV)
              <li>ACM Multimedia (ACM MM)
              <li>AAAI Conference on Artificial Intelligence (AAAI)
              <li>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
              <li>Computational Visual Media Conference (CVM)
              <li>Computer Graphics International (CGI)
              <li>Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
              <li>ACM international joint conference on pervasive and ubiquitous computing (UbiComp)
              <li>IEEE International Semantic Web Conference (ISWC)
              <li>IEEE International Symposium on Circuits and Systems (ISCAS)
              <li>International AAAI Conference on Web and Social Media (ICWSM) 
              <li>CCF BigData Conference (CCF BigData)
          </ul>
          <ul>
              <b>Reviewer for Fundings</b>
              <li>National Natural Science Foundation of China (NSFC)
              </ul>
                </div>
    </section> 
    
    
  </div>
  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/resume.min.js"></script>

</body>
<style>
  

.product-cell {
    /* width: 130%; */
    /* height: 90px; */
    margin-top: 15px;
  }

  .product-cell-img {
    /* float: left; */
    width: 240px;
    height: 120px;
  }

  .product-cell-text {
    display: inline;
  }

  .product-cell-num {
    /* float: left; */
    /* height: 70px; */
    color: black;
    margin-left: 15px;
    /* line-height: 70px; */
    text-align: center;
  }
</style>
</html>
