<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Add jQuery library -->
    <script type="text/javascript" src="lib/jquery-1.8.2.min.js"></script>
    <!-- Add mousewheel plugin (this is optional) -->
    <script type="text/javascript" src="lib/jquery.mousewheel-3.0.6.pack.js"></script>
    <!-- Add fancyBox main JS and CSS files -->
    <script type="text/javascript" src="source/jquery.fancybox.js?v=2.1.3"></script>
    <link rel="stylesheet" type="text/css" href="source/jquery.fancybox.css?v=2.1.2" media="screen" />
    <!-- javascript function-->
    <script type="text/javascript">
    $(document).ready(function() {
    $('.fancybox').fancybox();
    $(".fancybox-effects-c").fancybox({
      wrapCSS    : 'fancybox-custom',
      closeClick : true,
      openEffect : 'none',
      helpers : {
        title : {
          type : 'inside'
        },
        overlay : {
          css : {
            'background' : 'rgba(238,238,238,0.85)'
          }
        }
      }
        });
    });
    </script>
    <script type="text/javascript">
    function display(id){  
      var traget=document.getElementById(id);  
      if(traget.style.display=="none"){  
          traget.style.display="";  
      }else{  
          traget.style.display="none";  
        }  
        }  
        </script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>Yang Liu (刘阳) @SYSU</title>
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans:100,200,300,400,500,600,700,800,900"
      rel="stylesheet"> -->
    <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i"
      rel="stylesheet"> -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="shortcut icon" href="favicon.ico">
    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">
    <link href="css/index.css" rel="stylesheet">
</head>

<body id="page-top">

  <!-- Copyright ? 2008. Spidersoft Ltd -->
  <style>
  </style>
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
      <span class="d-block d-lg-none"></span>
      <span class="d-none d-lg-block">
        <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="./img/ArYanR2024.jpg" alt="">
      </span>
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#about">Introduction</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#News">News</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
        </li>
        <li class="nav-item">
          <a class="nav-link js-scroll-trigger" href="#activities">Activities</a>
        </li>          
      </ul>
    </div>
  </nav>

  <div class="container-fluid p-0">
    <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
      <div class="my-auto">
        <h2 class="mb-0">Yang Liu (刘阳)</h2>
        <div class="mb-5"> 
             IEEE/ACM Member, CCF/CAAI/CSIG Member<br/>
            <b>Email</b>: liuy856@mail.sysu.edu.cn<br/>
            <b>Office</b>:  School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China<br/>
        </div>
        <p class="mb-5">I am currently an associate professor at <a href="http://www.sysu-hcp.net/" target="_blank">HCP Lab</a>, <a href="http://sdcs.sysu.edu.cn/" target="_blank">School of Computer Science and Engineering</a>, 
            <a href="http://www.sysu.edu.cn/" target="_blank">Sun Yat-sen University</a>. From 2019-2024, I worked as a posdoctoral fellow (research fellow) at School of Computer Science and Engineering, Sun Yat-sen University, working with <a href="http://www.linliang.net/" target="_blank">Prof. Liang Lin</a>.
               I obtained my Ph.D degree of Telecommunications and Information Systems in June 2019 at <a href="http://www.phoenix-eye.com/" target="_blank">Phoenix-eye (XD145) Lab</a>, <a href="http://ste.xidian.edu.cn/" target="_blank">School of Telecommunications Engineering</a>,
              <a href="https://www.xidian.edu.cn/" target="_blank">Xidian University</a>, advised by <a href="http://web.xidian.edu.cn/zhylu/" target="_blank">Prof. Zhaoyang Lu</a>, <a href="https://web.xidian.edu.cn/jingli/" target="_blank">Prof. Jing Li</a> and <a href="https://teacher.nwpu.edu.cn/taoyang.html" target="_blank">Prof. Tao Yang</a>. I serve as the reviewer of IEEE T-PAMI, T-IP, T-NNLS, T-MM, T-CSVT, CVPR, ICCV, AAAI, ACM MM, ECCV, etc. <a href="https://www.sysu-hcp.net/faculty/liuyang.html" target="_blank">[中文版]</a><br/><br> 
            <font color="#FF0000"><b> Looking for self-motivated Masters, RAs, visiting students, and interns. Please drop me an email if interested.</b></font>
            <li>
               <b>Computer Vision</b>: Cross-modal Reasoning, Spatial-temporal Representation Learning
            </li>
            <li>
               <b>Machine Learning</b>: Causality Inference, Self-supervised Learning, Transfer Learning
            </li>
             <li>
               <b>Embodied AI</b>: Embodied Interaction, Embodied Manipulation, Robotic Control
            </li>
        </p>
        
              <!--<div class="my-auto">
             <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            Guangdong Basic and Applied Basic Research Foundation, ¥ 100K, 2023-2026, PI</li>
        </ul> 
                    <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            Science and Technology Projects in Guangzhou, ¥ 50K, 2022-2024, PI</li>
        </ul>  
          <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            National Key Research and Development Program of China, ¥ 5,000K, 2021-2025, Key Participant</li>
        </ul>  
        <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            National Natural Science Foundation of China, ¥ 300K, 2021-2024, PI</li>
        </ul>  
        <ul class="fa-ul mb-0">
         <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            Guangdong Basic and Applied Basic Research Foundation, ¥ 100K, 2021-2024, PI</li>
        </ul> 
       <ul class="fa-ul mb-0">  
        <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            General Program of China Postdoctoral Science Foundation, ¥ 80K, 2020-2022, PI</li>
        </ul> 
       <ul class="fa-ul mb-0">
        <li>
            <i class="fa-li fa fa-bank text-warning"></i>
            Fundamental Research Funds for the Central Universities, ¥ 120K, 2020-2022, PI</li>
        </ul>   
              </div>-->
        
<div class="my-auto">
            <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            CCF ChinaSoft 2023 Robotic Big Model and Embodied Intelligence Challenge, Third Prize, 2023.</li>
        </ul>
        <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            The Third Guangdong Province Young Computer Science Academic Show, First Prize, 2023.</li>
        </ul>
        <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            National Scholarship for PhD Students, 2018.</li>
        </ul>
        <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            National English Competition, Third Prize, 2013.</li>
        </ul>
       <ul class="fa-ul mb-0">
          <li>
            <i class="fa-li fa fa-trophy text-warning"></i>
            National Mathematical Modeling Competition, Second Prize, 2012.</li>
        </ul>
      </div>
        
        
        </br> 
        <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
            <a href="https://scholar.google.com/citations?user=l0z2QNQAAAAJ&hl=en" target="_blank">
              <i class="ai ai-google-scholar-square ai-3x"></i>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="https://github.com/YangLiu9208" target="_blank">
              <i class="fa fa-github-square fa-3x"></i>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="https://dblp.uni-trier.de/pid/51/3710-84.html" target="_blank">
              <i class="ai ai-dblp-square ai-3x"></i>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="https://www.researchgate.net/profile/Yang-Liu-30" target="_blank">
              <i class="ai ai-researchgate-square ai-3x"></i>
            </a>
          </li>
          <li class="list-inline-item">
            <a href="https://orcid.org/0000-0002-9423-9252" target="_blank">
              <i class="ai ai-orcid-square ai-3x"></i>
            </a>
          </li>
          </ul>
        <br>
        <div class="mb-5"> 
            <b>Related Link</b>: <a href="https://ccfddl.github.io/" target="_blank">AI Conference Calendar</a>, <a href="https://www.ccf.org.cn/c/2016-12-27/569124.shtml" target="_blank">CCF Rankings</a>
        </div>    
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=m&d=Ng_LP0UrQP5gWGJEym9UVNi8Up0eZ-cQXq9wH1Me7Bg"></script>
      </div>
    </section>
    
    <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="News">
      <div class="my-auto">
        <h3 class="mb-5">News</h3>
          <ul>
              <li>2024-07:  We release the <a href= "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List" target="_blank"> paper list </a> for Embodied AI ! 
              <li>2024-07:  One paper is accepted by ACM MM 2024! 
              <li>2024-06:  The book of multimodal large model <a href= "https://hcplab-sysu.github.io/Book-of-MLM/" target="_blank">《多模态大模型：新一代人工智能技术范式》</a> is selected for the Sun Yat-sen University Undergraduate Textbook Series!
              <li>2024-05:  One first-author T-PAMI paper is selected as the ESI Hot Cited Paper!
              <li>2024-05:  One first-author T-PAMI paper is selected as the ESI Highly Cited Paper!
              <li>2024-04:  The book of multimodal large model <a href= "https://hcplab-sysu.github.io/Book-of-MLM/" target="_blank">《多模态大模型：新一代人工智能技术范式》</a> is published!
              <li>2023-12:  I won the third prize of CCF ChinaSoft 2023 Robotic Big Model and Embodied Intelligence Challenge!
              <li>2023-11:  One first-author T-IP paper is selected as the ESI Hot Cited Paper!
              <li>2023-11:  One first-author T-IP paper is selected as the ESI Highly Cited Paper!
              <li>2023-10:  One accepted ACM MM 2023 paper is recommended as an oral!
              <li>2023-10:  An invention patent has been granted.
              <li>2023-10:  I have been selected as a member of the CSIG Visual Big Data Committee.
              <li>2023-07:  One paper is accepted by ACM MM 2023! 
              <li>2023-07:  Two papers are accepted by ICCV 2023! 
              <li>2023-06:  One paper is accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence! 
              <li>2023-06:  I won the first prize in the Third Guangdong Province Young Computer Science Academic Show. 
              <li>2023-06:  One paper is accepted by ICIP 2023. 
              <li>2023-04:  One paper is accepted by IJCAI 2023! 
              <li>2023-03:  The open-source framework for causal discovery and visual-linguistic reasoning <a href= "https://github.com/YangLiu9208/Causal-VLReasoning" target="_blank">Causal-VLReasoning</a> is online!
              <li>2023-03:  I become a member of CAAI (Chinese Association for Artificial Intelligence).
              <li>2023-03:  I become a member of CCF (China Computer Federation).
              <li>2023-03:  I am invited as the Program Committee (PC) Member for the <a href="http://www.cgs-network.org/cgi23/" target="_blank">Computer Graphics International (CGI) 2023</a>. 
              <li>2023-03:  One paper is accepted by Information Sciences. 
              <li>2022-12:  One <a href= "https://link.springer.com/article/10.1007/s11633-022-1362-z" target="_blank">review paper</a> about causal reasoning is reported in <a href="https://mp.weixin.qq.com/s/-OlJ44DWE6nuX_OVyykURw" target="_blank">social media</a> by Machine Intelligence Research.
              <li>2022-11:  I received the funding from Guangdong Basic and Applied Basic Research Foundation 2023.
              <li>2022-11:  One <a href="https://link.springer.com/article/10.1007/s11633-022-1362-z" target="_blank">review paper</a> for causal reasoning is published online by Machine Intelligence Research.  
              <li>2022-11:  I am invited as the Program Committee (PC) Member for the <a href="http://iccvm.org/2023/" target="_blank">Computational Visual Media Conference 2023</a>. 
              <li>2022-11:  I am invited as the reviewer of IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023.
              <li>2022-09:  I become a member of IEEE Industrial Electronics Society.
              <li>2022-08:  My <a href="https://github.com/YangLiu9208/TCGL" target="_blank">TCGL</a> paper published in IEEE T-IP is invited to be exhibited as the poster in <a href="http://valser.org/2022/#/poster" target="_blank">Valse 2022</a>.
              <li>2022-08:  One review paper about causal reasoning is accepted by Machine Intelligence Research. 
              <li>2022-08:  I am invited as the Program Committee (PC) Member for the AAAI 2023. 
              <li>2022-06:  I become a member of CSIG.
              <li>2022-05:  One paper is accepted by IEEE Transactions on Industrial Informatics. 
              <li>2022-04:  I received the funding from Science and Technology Projects in Guangzhou.
              <li>2022-03:  One paper is accepted by CVPR 2022 as an oral presentation. 
              <li>2022-01:  One paper is accepted by IEEE Transactions on Image Processing. <a href="v" target="_blank">Code</a> is available.
              <li>2022-01:  One paper is accepted by ICASSP 2022. 
              <li>2022-01:  The code for <a href="https://github.com/YangLiu9208/TCGL" target="_blank">TCGL</a> is available.
              <li>2021-12:  I am invited as the reviewer of Chinese Conference on Pattern Recognition and Computer Vision (PRCV) 2022.
              <li>2021-11:  I am invited as the reviewer of IEEE/CVF Conference on Computer Vision and Pattern Recognition 2022.
              <li>2021-10:  I start working as a research associate professor at Sun-Yat-Sen University.
              <li>2021-05:  One paper is accepted by IEEE Transactions on Image Processing. <a href="https://yangliu9208.github.io/SAKDN/" target="_blank">Code</a> is available.
              <li>2021-04:  The code for SAKDN is available.
              <li>2021-02:  I become a Member of the IEEE.
              <li>2020-12:  I am invitied as the reviewing expert for NSFC.
              <li>2020-11:  I received the funding from National Natural Science Foundation of Guangdong Province of China 2021.
              <li>2020-09:  I received the funding from National Natural Science Foundation of China 2021.
              <li>2020-06:  I received the funding from China Postdoctoral Science Foundation 2020.
              <li>2020-03:  I received the funding from Fundamental Research of the Central Universities 2020.
              <li>2020-01:  My Ph.D Thesis (psw：ithd) is available now.
              <li>2019-10:  Codes and datasets for IEEE TIP is available.
              <li>2019-10:  One paper is accepted by IEEE Transactions on Image Processing.
              <li>2019-08:  I start working as a postdoctoral fellow at Sun-Yat-Sen University.
              <li>2019-06:  I obtain my Ph.D degree from Xidian University.
          </ul>
      </div>
    </section>    
    
    <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div>    
         <h3 class="mb-5">Open-source Framework</h3>   
            <ol style="list-style-type:none">
                           <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
             <li><img class="product-cell-img" src="img/CausalVLR.gif" style="width=320"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal Reasoning</b></br> 
               <b>Yang Liu</b>, Weixing Chen, Guanbin Li, Liang Lin
                 <table class="imgtable">
                 <tr>
                    <td>
                         <p class="pub_link">[<a href="https://github.com/HCPLab-SYSU/CausalVLR" target="_blank">CausalVLR</a>] 
                                                [<a href= "https://arxiv.org/pdf/2306.17462.pdf" target="_blank">Paper</a>] 
                                                [<a  onclick="display('Causal')" style="cursor:pointer;">BibTex</a>]
                             </p>
                     </td>
                 </tr>
                 </table>
               <div>
                        <pre id="Causal" style="display: none;">
                            @article{CausalVLR,
                              title={CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal Reasoning},
                              author={Liu, Yang and Chen, Weixing and Li, Guanbin and Lin, Liang},
                              journal={arXiv preprint arXiv:2306.17462},
                              year={2023}
                            }
                            }
                          </pre>
                         </div>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/CausalVLR"></a>
                 <li>CausalVLR is a python open-source framework for causal relation discovery, causal inference that implements state-of-the-art causality learning algorithms for various visual-linguistic reasoning tasks, such as VQA, embodied interaction, model robustness, medical report generation, etc.</li>
             </div>
           </div>  
                   </ol>  
      </div>
   
                <div>  
                 <h3 class="mb-5">Demos of Embodied Agents</h3>   
                    <ol style="list-style-type:none">

                                                             <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                    <li><img class="product-cell-img" src="img/demo_inspection.gif" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Robotic Inspection in Extreme Environments</b></br> 
                 <li>The robotic dog actively checks the surrounding states of extreme environments with the dark light condiction, including checking the machine states, detecting anomaly intrusion and human-robot dialog.  </li>
             </div>
           </div>  
                        
                                     <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                                                      <li><img class="product-cell-img" src="img/Demo_Public_0421.gif" style="width=240"></li>   
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Embodied Interactive Robot</b></br> 
                 <li>We implemtent and integrate active visual sensing, visual navigation, embodied dialog, robotic manipulation and robotic arm controling algorithms into the robotic dog. The robotic dog can provide vairous services for humans in complex and dynamic environments.  </li>
             </div>
           </div>  
                           <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                                     <li><img class="product-cell-img" src="img/EmbodiedAgent_low.gif" style="width=240"></li>   
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Embodied Interactive Agent for the Coffee Scene</b></br> 
           <li> The multimodal embodied interactive agent (MEIA) can translate high-level tasks expressed in natural language into a sequence of executable actions. The MEIA can generate executable action plans based on diverse requirements and the robot's capabilities. Furthermore, we construct an embodied question answering dataset based on a dynamic virtual cafe environment with the large language model.
                The MEIA obtained the third prize of CCF ChinaSoft 2023 Robotic Big Model and Embodied Intelligence Challenge. </li>
             </div>
           </div>  
      </div>
         </ol>
        <div>  

            
                 <h3 class="mb-5">Book</h3>   
                                  <ol style="list-style-type:none">
                           <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                                <li><img class="product-cell-img" src="img/PHE.jpg" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Multimodal Large Models: The New Paradigm of Artificial General Intelligence</b></br> 
                  <b>《多模态大模型：新一代人工智能技术范式》</b></br> 
                 <b>Publishing House of Electronics Industry (PHE), ISBN 978-7-121-47547-4, 2024.</b></br>
                <b>电子工业出版社，ISBN 978-7-121-47547-4, 2024.</b></br>
               <b>Yang Liu</b>, Liang Lin</br>
               <b>刘阳</b>, 林倞</br>
               <font color="#FF0000"><b>Sun Yat-sen University Undergraduate Textbook Series</b></font>
                             <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://hcplab-sysu.github.io/Book-of-MLM/" target="_blank">Resource Page (在线资源)</a>] [<a href= "https://item.jd.com/10100489294930.html" target="_blank">JD Page (京东商城链接)</a>][<a href= "https://mp.weixin.qq.com/s/WHYy-dlJl6V4TQoZxWIiYQ" target="_blank">媒体报道</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                             <a href="https://github.com/HCPLab-SYSU/Book-of-MLM/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/Book-of-MLM"></a>
      <a href="https://github.com/HCPLab-SYSU/Book-of-MLM/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/Book-of-MLM"></a>
      <a href="https://github.com/HCPLab-SYSU/Book-of-MLM/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/Book-of-MLM"></a>
             </div>
           </div>  
      </div>
 </ol>
        
        <h3 class="mb-5">Selected Papers</h3>   
                    <ol style="list-style-type:none">

                                                    <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.gif"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/Embodied_survey.jpg" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI</b></br>
               <b>Yang Liu</b>, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, Liang Lin<br>
                <b>arXiv:2407.06886.</b> 
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://arxiv.org/pdf/2407.06886" target="_blank">Paper</a>] 
                         [<a href="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List" target="_blank">Embodied AI Paper List</a>] 
                        [<a href="https://mp.weixin.qq.com/s/nRpIRi6dIEwG2NLgVDDkiQ" target="_blank">机器之心中文解读</a>] 
                        [<a  onclick="display('embodied_survey')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="embodied_survey" style="display: none;">
                                @article{liu2024aligning,
                                  title={Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI},
                                  author={Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
                                  journal={arXiv preprint arXiv:2407.06886},
                                  year={2024}
                                }
                          </pre>
                         </div>
      <a href="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/Embodied_AI_Paper_List"></a>
      <a href="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/Embodied_AI_Paper_List"></a>
      <a href="https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/Embodied_AI_Paper_List"></a>
           </div> 
           </div> 
                        
                            <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.gif"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/VLCIR.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering</b></br>
               <b>Yang Liu</b>, Guanbin Li, Liang Lin<br>
                <b>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023.</b>  (<font color="#FF0000"><b>ESI Highly Cited & Hot Paper</b></font>)
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/10146482" target="_blank">Paper</a>] 
                         [<a href= "https://mp.weixin.qq.com/s/RRVIACXRLA0-nePQO5bY6g" target="_blank">中文解读</a>]    
                         [<a href="https://github.com/HCPLab-SYSU/CMCIR" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('CMCIR')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="CMCIR" style="display: none;">
                                  @article{liu2022cross,
                                         author={Liu, Yang and Li, Guanbin and Lin, Liang},
                                          journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
                                          title={Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering}, 
                                          year={2023},
                                          volume={},
                                          number={},
                                          pages={1-17},
                                          doi={10.1109/TPAMI.2023.3284038}
                                            }
                          </pre>
                         </div>
      <a href="https://github.com/HCPLab-SYSU/CMCIR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/CMCIR"></a>
      <a href="https://github.com/HCPLab-SYSU/CMCIR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/CMCIR"></a>
      <a href="https://github.com/HCPLab-SYSU/CMCIR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/CMCIR"></a>
      <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:LhH-TYMQEocC" target="_blank"></a>
           </div> 
           </div> 
                                                                                                        <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/CRS.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Diversity Matters: User-Centric Multi-Interest Learning for Conversational Movie Recommendation</b></br>
              Yongsen Zheng, Guohua Wang, <b>Yang Liu</b>, Liang Lin<br>
                 	 <b>ACM International Conference on Multimedia (ACM MM), 2024.</b>
                 <table class="imgtable">
                 <tr>
                    <td> 
                         <p class="pub_link">[<a href= "https://yangliu9208.github.io/" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/" target="_blank">Code</a>] 
                        [<a  onclick="display('CRS')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="CRS" style="display: none;">
                          </pre>
                         </div>
           </div> 
           </div> 

                                                        <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/MEIA.jpg"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/MEIA.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>MEIA: Multimodal Embodied Perception and Interaction in Unknown Environments</b></br>
              <b>Yang Liu</b>, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, Liang Lin<br>
                 	 <b>arXiv:2402.00290</b>
                 <table class="imgtable">
                 <tr>
                    <td> 
                         <p class="pub_link">[<a href= "https://arxiv.org/pdf/2402.00290v2" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('MEIA')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="MEIA" style="display: none;">
                            @article{liu2024multimodal,
                                      title={MEIA: Multimodal Embodied Perception and Interaction in Unknown Environments},
                                      author={Liu, Yang and Song, Xinshuai and Jiang, Kaixuan and Chen, Weixing and Luo, Jingzhou and Li, Guanbin and Lin, Liang},
                                      journal={arXiv preprint arXiv:2402.00290},
                                      year={2024}
                                    }
                          </pre>
                         </div>
           </div> 
           </div> 

                                                                                <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/MEIA.jpg"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/VidMaestro.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>VidMaestro: Towards Photo-realistic and High-dynamic Video Generations</b></br>
              Binbin Yang, Kangyang Xie, Xinyu Xiao, Meng Wang, <b>Yang Liu</b>, Jingdong Chen, Ming Yang, Liang Lin<br>
                 	 <b>Submitted to TMM, arXiv, 2024</b>
                 <table class="imgtable">
                 <tr>
                    <td> 
                         <p class="pub_link">[<a href= "https://www.sysu-hcp.net/projects/cv/146.html" target="_blank">Project</a>] 
                         [<a href="https://www.sysu-hcp.net/projects/cv/146.html" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('VidMaestro')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="MEIA" style="display: none;">
                          </pre>
                         </div>
           </div> 
           </div> 
                        
                                                        <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.gif"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/CaCoCoT.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>CausalGPT: Illuminating Faithfulness and Causality for Knowledge-based Reasoning with LLMs</b></br>
              Ziyi Tang, Ruilin Wang, Weixing Chen, Yongsen Zheng, <b>Yang Liu</b>, Keze Wang, Tianshui Chen, Liang Lin<br>
                 <b>arXiv:2308.11914</b>
                 <table class="imgtable">
                 <tr>
                    <td> 
                        <p class="pub_link">[<a href= "https://arxiv.org/pdf/2308.11914.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/HCPLab-SYSU/CausalVLR" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('CausalGPT')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="CausalGPT" style="display: none;">
                                @article{tang2023towards,
                                  title={Towards causalgpt: A multi-agent approach for faithful knowledge reasoning via promoting causal consistency in llms},
                                  author={Tang, Ziyi and Wang, Ruilin and Chen, Weixing and Wang, Keze and Liu, Yang and Chen, Tianshui and Lin, Liang},
                                  journal={arXiv preprint arXiv:2308.11914},
                                  year={2023}
                                }
                          </pre>
                         </div>
      <a href=https://github.com/HCPLab-SYSU/CausalVLR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:S16KYo8Pm5AC" target="_blank"></a>
           </div> 
           </div> 

                                                                                <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/ODMixer.jpg" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>ODMixer: Fine-grained Spatial-temporal MLP for Metro Origin-Destination Prediction</b></br>
              <b>Yang Liu</b>, Binglin Chen, Yongsen Zheng, Lechao Cheng, Guanbin Li, Liang Lin<br>
                 	 <b>Submitted to TKDE, arXiv:2404.15734</b>
                 <table class="imgtable">
                 <tr>
                    <td> 
                         <p class="pub_link">[<a href= "https://arxiv.org/pdf/2404.15734" target="_blank">Paper</a>] 
                         [<a href="https://github.com/KLatitude/ODMixer" target="_blank">Code</a>] 
                        [<a  onclick="display('ODMixer')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="ODMixer" style="display: none;">
                                @article{liu2024fine,
                                  title={ODMixer: Fine-grained Spatial-temporal MLP for Metro Origin-Destination Prediction},
                                  author={Liu, Yang and Chen, Binglin and Zheng, Yongsen and Cheng, Lechao and Li, Guanbin and Lin, Liang},
                                  journal={arXiv preprint arXiv:2404.15734},
                                  year={2024}
                                }
                          </pre>
                         </div>
                       <a href=https://github.com/KLatitude/ODMixer/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/KLatitude/ODMixer"></a>
      <a href="https://github.com/KLatitude/ODMixer/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/KLatitude/ODMixer"></a>
      <a href="https://github.com/KLatitude/ODMixer/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/KLatitude/ODMixer"></a>
           </div> 
           </div> 
              
                                          <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.gif"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/VLCI.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Cross-Modal Causal Intervention for Medical Report Generation</b></br>
               Weixing Chen, <b>Yang Liu</b><sup>✉</sup>, Ce Wang, Jiarui Zhu, Shen Zhao, Guanbin Li, Cheng-Lin Liu, Liang Lin<br>
                 <b>Arxiv, 2024. </b>
                 <table class="imgtable">
                 <tr>
                    <td> 
                        [<a href= "https://arxiv.org/pdf/2303.09117" target="_blank">Paper</a>] 
                         [<a href="https://github.com/WissingChen/VLCI" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('VLCI')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="VLCI" style="display: none;">
                                @article{chen2023visual,
                                  title={Visual-linguistic causal intervention for radiology report generation},
                                  author={Chen, Weixing and Liu, Yang and Wang, Ce and Li, Guanbin and Zhu, Jiarui and Lin, Liang},
                                  journal={arXiv preprint arXiv:2303.09117},
                                  year={2023}
                                }
                          </pre>
                         </div>
      <a href=https://github.com/WissingChen/VLCI/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/WissingChen/VLCI"></a>
      <a href="https://github.com/WissingChen/VLCI/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/WissingChen/VLCI"></a>
      <a href="https://github.com/WissingChen/VLCI/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/WissingChen/VLCI"></a>
      <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:J-pR_7NvFogC" target="_blank"></a>
           </div> 
           </div> 
                                                     <!-- 文章序列10 -->
          
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/VCSR.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Visual Causal Scene Refinement for Video Question Answering</b></br> 
               Yushen Wei*, <b>Yang Liu*</b>, Hong Yan, Guanbin Li, Liang Lin<sup>✉</sup><br>
                                              <b>ACM International Conference on Multimedia (ACM MM), 2023. </b>  (<font color="#FF0000"><b>Oral</b></font>)
                                                              <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://arxiv.org/pdf/2305.04224.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/HCPLab-SYSU/CausalVLR" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('VCSR')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="VCSR" style="display: none;">
                            @inproceedings{10.1145/3581783.3611873,
                            author = {Wei, Yushen and Liu, Yang and Yan, Hong and Li, Guanbin and Lin, Liang},
                            title = {Visual Causal Scene Refinement for Video Question Answering},
                            year = {2023},
                            isbn = {9798400701085},
                            publisher = {Association for Computing Machinery},
                            address = {New York, NY, USA},
                            url = {https://doi.org/10.1145/3581783.3611873},
                            doi = {10.1145/3581783.3611873},
                            pages = {377–386},
                            numpages = {10},
                            keywords = {causal reasoning, video question answering, cross-modal},
                            location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
                            series = {MM '23}
                            }
                          </pre>
                         </div>
    <a href="https://github.com/HCPLab-SYSU/CausalVLR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HCPLab-SYSU/CausalVLR"></a>
      <a href="https://github.com/HCPLab-SYSU/CausalVLR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HCPLab-SYSU/CausalVLR"></a>
    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:mlAyqtXpCwEC" target="_blank"></a>
             </div>
           </div>
   
                                                   <!-- 文章序列10 -->
          
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/SkeletonMAE.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training</b></br> 
               Hong Yan, <b>Yang Liu</b><sup>✉</sup>, Yushen Wei, Zhen Li, Guanbin Li, Liang Lin<br>
                                              <b>IEEE/CVF International Conference on Computer Vision (ICCV), 2023.</b> 
                                                              <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_SkeletonMAE_Graph-based_Masked_Autoencoder_for_Skeleton_Sequence_Pre-training_ICCV_2023_paper.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/HongYan1123/SkeletonMAE" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('SkeletonMAE')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="SkeletonMAE" style="display: none;">
                            @inproceedings{yan2023skeletonmae,
                              title={Skeletonmae: graph-based masked autoencoder for skeleton sequence pre-training},
                              author={Yan, Hong and Liu, Yang and Wei, Yushen and Li, Zhen and Li, Guanbin and Lin, Liang},
                              booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
                              pages={5606--5618},
                              year={2023}
                            }
                          </pre>
                         </div>
                       <a href="https://github.com/HongYan1123/SkeletonMAE/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/HongYan1123/SkeletonMAE"></a>
      <a href="https://github.com/HongYan1123/SkeletonMAE/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/HongYan1123/SkeletonMAE"></a>
      <a href="https://github.com/HongYan1123/SkeletonMAE/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/HongYan1123/SkeletonMAE"></a>
    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:_FM0Bhl9EiAC" target="_blank"></a>
             </div>
           </div>  

                                                                 <!-- 文章序列10 -->
          
          <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/ESL.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Enhanced Soft Label for Semi-Supervised Semantic Segmentation</b></br> 
               Jie Ma, Chuan Wang, <b>Yang Liu</b>, Liang Lin, Guanbin Li<br>
                                              <b>IEEE/CVF International Conference on Computer Vision (ICCV), 2023.</b> 
                                                              <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Enhanced_Soft_Label_for_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/unrealMJ/ESL" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('ESL')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="ESL" style="display: none;">
                                @inproceedings{ma2023enhanced,
                                  title={Enhanced Soft Label for Semi-Supervised Semantic Segmentation},
                                  author={Ma, Jie and Wang, Chuan and Liu, Yang and Lin, Liang and Li, Guanbin},
                                  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
                                  pages={1185--1195},
                                  year={2023}
                                }
                          </pre>
                         </div>
                       <a href="https://github.com/unrealMJ/ESL/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/unrealMJ/ESL"></a>
      <a href="https://github.com/unrealMJ/ESL/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/unrealMJ/ESL"></a>
      <a href="https://github.com/unrealMJ/ESL/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/unrealMJ/ESL"></a>
        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:FAceZFleit8C" target="_blank"></a>
             </div>
           </div>
              
                                                       <!-- 文章序列11 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                 <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/VLCIR.mp4"> <source src="img/VLCIR.mp4" type="video/mp4"></video></li>--> 
                <li><img class="product-cell-img" src="img/DenseLight.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>DenseLight: Efficient Control for Large-scale Traffic Signals with Dense Feedback</b></br>
               Junfan Lin, Yuying Zhu, Lingbo Liu, <b>Yang Liu</b><sup>✉</sup>, Guanbin Li, Liang Lin<br>
                <b>International Joint Conference on Artificial Intelligence (IJCAI), 2023.</b> 
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://yangliu9208.github.io/home/" target="_blank">Paper</a>] 
                         [<a href=" https://github.com/junfanlin/DenseLight" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('DenseLight')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="DenseLight" style="display: none;">
                            @inproceedings{ijcai2023p672,
                              title     = {DenseLight: Efficient Control for Large-scale Traffic Signals with Dense Feedback},
                              author    = {Lin, Junfan and Zhu, Yuying and Liu, Lingbo and Liu, Yang and Li, Guanbin and Lin, Liang},
                              booktitle = {Proceedings of the Thirty-Second International Joint Conference on
                                           Artificial Intelligence, {IJCAI-23}},
                              publisher = {International Joint Conferences on Artificial Intelligence Organization},
                              editor    = {Edith Elkind},
                              pages     = {6058--6066},
                              year      = {2023},
                              month     = {8},
                              note      = {AI for Good},
                              doi       = {10.24963/ijcai.2023/672},
                              url       = {https://doi.org/10.24963/ijcai.2023/672},
                            }
                                                      </pre>
                         </div>
                     <a href="https://github.com/junfanlin/DenseLight/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/junfanlin/DenseLight"></a>
      <a href="https://github.com/junfanlin/DenseLight/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/junfanlin/DenseLight"></a>
      <a href="https://github.com/junfanlin/DenseLight/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/junfanlin/DenseLight"></a>
    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:olpn-zPbct0C" target="_blank"></a>
           </div> 
           </div> 
                        
                               <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                 <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/TII.gif"> <source src="img/TII.mp4" type="video/mp4"></video></li>--> 
               <li><img class="product-cell-img" src="img/HORLN.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Hybrid-Order Representation Learning for Electricity Theft Detection</b></br>  
                Yuying Zhu, Yang Zhang, Lingbo Liu, <b>Yang Liu</b><sup>✉</sup>, Guanbin Li, Mingzhi Mao, Liang Lin<br>
                <b>IEEE Transactions on Industrial Informatics (T-II), 2023.</b>
                                <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/9785914" target="_blank">Paper</a>] 
                         [<a href="https://github.com/GillianZhu/HORLN" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('HORLN')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="HORLN" style="display: none;">
                    @article{zhu2022hybrid,
                      title={Hybrid-Order Representation Learning for Electricity Theft Detection},
                      author={Zhu, Yuying and Zhang, Yang and Liu, Lingbo and Liu, Yang and Li, Guanbin and Mao, Mingzhi and Lin, Liang},
                      journal={IEEE Transactions on Industrial Informatics},
                      volume={19},
                      number={2},
                      pages={1248--1259},
                      year={2023},
                      publisher={IEEE}
                    }
                          </pre>
                             </div>
      <a href="https://github.com/GillianZhu/HORLN/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/GillianZhu/HORLN"></a>
      <a href="https://github.com/GillianZhu/HORLN/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/GillianZhu/HORLN"></a>
      <a href="https://github.com/GillianZhu/HORLN/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/GillianZhu/HORLN"></a>
                            <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:F9fV5C73w3QC" target="_blank"></a>
             </div>
           </div> 
              
                                                                   <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/IS.gif"> <source src="img/IS.mp4" type="video/mp4"></video></li>--> 
              <li><img class="product-cell-img" src="img/POI.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Urban Regional Function Guided Traffic Flow Prediction</b> </br> 
               Kuo Wang, Lingbo Liu, <b>Yang Liu</b><sup>✉</sup>, Guanbin Li, Liang Lin<br>
                <b>Information Sciences (INS), 2023.</b> 
                                               <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://www.sciencedirect.com/science/article/pii/S0020025523004334" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('TFP')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="TFP" style="display: none;">
                                  @article{TFP,
                                        title = {Urban regional function guided traffic flow prediction},
                                        journal = {Information Sciences},
                                        volume = {634},
                                        pages = {308-320},
                                        year = {2023},
                                        issn = {0020-0255},
                                        doi = {https://doi.org/10.1016/j.ins.2023.03.109},
                                        url = {https://www.sciencedirect.com/science/article/pii/S0020025523004334},
                                        author = {Kuo Wang and LingBo Liu and Yang Liu and GuanBin Li and Fan Zhou and Liang Lin},
                                            }
                          </pre>
                             </div>
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:OP4eGU-M3BUC" target="_blank"></a>
             </div>
           </div> 

                                                                               <!-- 文章序列10 -->
          
          <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
               <li><img class="product-cell-img" src="img/DADA.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Dual adversarial adaptation for cross-device real-world image super-resolution</b></br> 
               Xiaoqian Xu, Pengxu Wei, Weikai Chen, <b>Yang Liu</b>, Mingzhi Mao, Liang Lin, Guanbin Li<br>
                                              <b>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</b> (<font color="#FF0000"><b>Oral</b></font>)
                                                              <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Dual_Adversarial_Adaptation_for_Cross-Device_Real-World_Image_Super-Resolution_CVPR_2022_paper.pdf" target="_blank">Paper</a>] 
                         [<a href="https://github.com/lonelyhope/DADA" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('DADA')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="DADA" style="display: none;">
                            @inproceedings{xu2022dual,
                              title={Dual adversarial adaptation for cross-device real-world image super-resolution},
                              author={Xu, Xiaoqian and Wei, Pengxu and Chen, Weikai and Liu, Yang and Mao, Mingzhi and Lin, Liang and Li, Guanbin},
                              booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                              pages={5667--5676},
                              year={2022}
                            }
                          </pre>
                             </div>
                       <a href="https://github.com/lonelyhope/DADA/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/lonelyhope/DADA"></a>
      <a href="https://github.com/lonelyhope/DADA/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/lonelyhope/DADA"></a>
      <a href=https://github.com/lonelyhope/DADA/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/lonelyhope/DADA"></a>
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:VaXvl8Fpj5cC" target="_blank"></a>
             </div>
           </div>
              
                    <!-- 文章序列10 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                 <li><video  muted="muted" width="240" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/MIR.mp4"> <source src="img/MIR.mp4" type="video/mp4"></video></li> 
                <!-- <li><img class="product-cell-img" src="img/CSTL.png"></li>-->
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Causal Reasoning Meets Visual Representation Learning: A Prospective Study</b></br>   
                <b>Yang Liu</b>, Yushen Wei, Hong Yan, Guanbin Li, Liang Lin<br>
                <b>Machine Intelligence Research (MIR), 2022.</b> (<font color="#FF0000"><b>Top-10 Downloads</b></font>)
                                <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://link.springer.com/article/10.1007/s11633-022-1362-z" target="_blank">Paper</a>] 
                         [<a href= " https://arxiv.org/abs/2204.12037" target="_blank">Arxiv (Keep Updating)</a>]   
                         [<a href="https://youtu.be/2lfNaTkcTHI" target="_blank">Video (Youtube)</a>] 
                         [<a href="https://www.bilibili.com/video/BV1gP411o7m2" target="_blank">Video (BiliBili)</a>] 
                         [<a href="https://mp.weixin.qq.com/s/-OlJ44DWE6nuX_OVyykURw" target="_blank">中文解读</a>] 
                        [<a  onclick="display('Review')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="Review" style="display: none;">
                                @article{liu2022causal,
                                  title={Causal Reasoning Meets Visual Representation Learning: A Prospective Study},
                                  author={Liu, Yang and Wei, Yu-Shen and Yan, Hong and Li, Guan-Bin and Lin, Liang},
                                  journal={Machine Intelligence Research},
                                  pages={1--27},
                                  year={2022},
                                  publisher={Springer}
                                }
                          </pre>
                             </div>
                    <a href="https://github.com/YangLiu9208/MIR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/MIR"></a>
      <a href="https://github.com/YangLiu9208/MIR/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/MIR"></a>
      <a href="https://github.com/YangLiu9208/MIR/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/MIR"></a>
                        <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=l0z2QNQAAAAJ&citation_for_view=l0z2QNQAAAAJ:a3BOlSfXSfwC" target="_blank"></a>
             </div>
           </div> 


                                                                                               <!-- 文章序列10 -->
          <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/IS.gif"> <source src="img/IS.mp4" type="video/mp4"></video></li>--> 
              <li><img class="product-cell-img" src="img/VSAR.png" style="width=240"></li>
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>Cross-modal knowledge distillation for Vision-to-Sensor action recognition</b> </br> 
              Jianyuan Ni, Raunak Sarbajna, <b>Yang Liu</b>, Anne HH Ngu, Yan Yan<br>
                <b>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022.</b> 
                                               <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/abstract/document/9746752" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('VSAR')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="TFP" style="display: none;">
                            @inproceedings{ni2022cross,
                              title={Cross-modal knowledge distillation for vision-to-sensor action recognition},
                              author={Ni, Jianyuan and Sarbajna, Raunak and Liu, Yang and Ngu, Anne HH and Yan, Yan},
                              booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
                              pages={4448--4452},
                              year={2022},
                              organization={IEEE}
                            }
                          </pre>
                             </div>
             </div>
           </div>
                                   <!-- 文章序列9 -->
           <div class="product-cell row align-items-center justify-content-center">
             <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                 <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/TCGL.gif"> <source src="img/TCGL.mp4" type="video/mp4"></video></li>--> 
                 <li><img class="product-cell-img" src="img/TCGL.png" style="width=240"></li> 
             </div>
             <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
               <b>TCGL: Temporal Contrastive Graph for Self-supervised Video Representation Learning</b></br>  
                <b>Yang Liu</b>, Keze Wang, Lingbo Liu, Haoyuan Lan, Liang Lin<br>
                <b>IEEE Transactions on Image Processing</b> (<b>T-IP</b>), <b>2022.</b> (<font color="#FF0000"><b>ESI Highly Cited & Hot Paper</b></font>)
                                <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/9713748" target="_blank">Paper</a>] 
                         [<a href="https://github.com/YangLiu9208/TCGL/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('TCGL')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="TCGL" style="display: none;">
                            @article{liu2022tcgl,
                              title={TCGL: Temporal Contrastive Graph for Self-Supervised Video Representation Learning},
                              author={Liu, Yang and Wang, Keze and Liu, Lingbo and Lan, Haoyuan and Lin, Liang},
                              journal={IEEE Transactions on Image Processing},
                              volume={31},
                              pages={1978--1993},
                              year={2022},
                              publisher={IEEE}
                            }
                          </pre>
                             </div>
      <a href="https://github.com/YangLiu9208/TCGL/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/TCGL"></a>
      <a href="https://github.com/YangLiu9208/TCGL/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/TCGL"></a>
      <a href="https://github.com/YangLiu9208/TCGL/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/TCGL"></a>
             </div>
           </div> 

                     <!-- 文章序列7 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!-- <li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/SAKDN.gif"> <source src="img/SAKDN.mp4" type="video/mp4"></video></li> --> 
                <li><img class="product-cell-img" src="img/SAKDN.png" style="width=240"></li>
              </div>
              <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
                <b>Semantics-aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition</b></br>  
                  <b>Yang Liu</b>, Keze Wang, Guanbin Li, Liang Lin<br>
                  <b>IEEE Transactions on Image Processing</b> (<b>T-IP</b>), <b>2021</b>.
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/9451581" target="_blank">Paper</a>] 
                         [<a href="https://github.com/YangLiu9208/SAKDN" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('SAKDN')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="SAKDN" style="display: none;">
                            @article{liu2021semantics,
                              title={Semantics-aware adaptive knowledge distillation for sensor-to-vision action recognition},
                              author={Liu, Yang and Wang, Keze and Li, Guanbin and Lin, Liang},
                              journal={IEEE Transactions on Image Processing},
                              volume={30},
                              pages={5573--5588},
                              year={2021},
                              publisher={IEEE}
                            }
                          </pre>
                             </div>
      <a href="https://github.com/YangLiu9208/SAKDN/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/SAKDN"></a>
      <a href="https://github.com/YangLiu9208/SAKDN/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/SAKDN"></a>
      <a href="https://github.com/YangLiu9208/SAKDN/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/SAKDN"></a>
              </div>
            </div>

                 <!-- 文章序列6 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                   <!-- <li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/DIVAFN.gif"> <source src="img/DIVAFN.mp4" type="video/mp4"></video></li>--> 
                <li><img class="product-cell-img" src="img/TIP.png" style="width=240"></li> 
              </div>
              <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
                <b>Deep Image-to-Video Adaptation and Fusion Networks for Action Recognition</b></br>  
                  <b>Yang Liu</b>, Zhaoyang Lu, Jing Li, Tao Yang, Chao Yao<br>
                  <b>IEEE Transactions on Image Processing</b> (<b>T-IP</b>), <b>2020</b>. 
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/8931264" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/DIVAFN/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('DIVAFN')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="DIVAFN" style="display: none;">
                            @article{liu2019deep,
                              title={Deep image-to-video adaptation and fusion networks for action recognition},
                              author={Liu, Yang and Lu, Zhaoyang and Li, Jing and Yang, Tao and Yao, Chao},
                              journal={IEEE Transactions on Image Processing},
                              volume={29},
                              pages={3168--3182},
                              year={2019},
                              publisher={IEEE}
                            }
                          </pre>
                             </div>
      <a href="https://github.com/YangLiu9208/DIVAFN/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/DIVAFN"></a>
      <a href="https://github.com/YangLiu9208/DIVAFN/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/DIVAFN"></a>
      <a href="https://github.com/YangLiu9208/DIVAFN/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/DIVAFN"></a>
              </div>
            </div> 


            <!-- 文章序列5 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                                     <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/TCSVT.mp4"> <source src="img/TCSVT.mp4" type="video/mp4"></video></li>-->
                 <li><img class="product-cell-img" src="img/TCSVT.png" style="width=240"></li>
              </div>
              <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
                <b>Hierarchically Learned View-Invariant Representations for Cross View Action Recognition</b></br> 
                  <b>Yang Liu</b>, Zhaoyang Lu, Jing Li, Tao Yang<br>
                  <b>IEEE Transactions on Circuits and Systems for Video Technology</b> (<b>T-CSVT</b>), <b>2019</b>.
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/8453034" target="_blank">Paper</a>] 
                         [<a href="https://github.com/YangLiu9208/JSRDA/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('JSRDA')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="JSRDA" style="display: none;">
                            @article{liu2018hierarchically,
                              title={Hierarchically learned view-invariant representations for cross-view action recognition},
                              author={Liu, Yang and Lu, Zhaoyang and Li, Jing and Yang, Tao},
                              journal={IEEE Transactions on Circuits and Systems for Video Technology},
                              volume={29},
                              number={8},
                              pages={2416--2430},
                              year={2018},
                              publisher={IEEE}
                            }
                          </pre>
                             </div>
      <a href="https://github.com/YangLiu9208/JSRDA/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/JSRDA"></a>
      <a href="https://github.com/YangLiu9208/JSRDA/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/JSRDA"></a>
      <a href="https://github.com/YangLiu9208/JSRDA/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/JSRDA"></a>  
              </div>
            </div>      
              
            <!-- 文章序列4 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                   <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/SPL.gif"> <source src="img/SPL.mp4" type="video/mp4"></video></li> -->
               <li><img class="product-cell-img" src="img/SPL.png" style="width=240"></li>
              </div>
              <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
                <b>Global Temporal Representation based CNNs for Infrared Action Recognition</b></br>  
                  <b>Yang Liu</b>, Zhaoyang Lu, Jing Li, Tao Yang, Chao Yao<br>
                  <b>IEEE Signal Processing Letters</b> (<b>SPL</b>), <b>2018</b>.
                 <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://ieeexplore.ieee.org/document/8332532" target="_blank">Paper</a>] 
                         [<a href="https://yangliu9208.github.io/TSTDDs/" target="_blank">Code & Dataset</a>] 
                        [<a  onclick="display('TSTDDs')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="TSTDDs" style="display: none;">
                            @article{liu2018global,
                              title={Global temporal representation based cnns for infrared action recognition},
                              author={Liu, Yang and Lu, Zhaoyang and Li, Jing and Yang, Tao and Yao, Chao},
                              journal={IEEE Signal Processing Letters},
                              volume={25},
                              number={6},
                              pages={848--852},
                              year={2018},
                              publisher={IEEE}
                            }
                          </pre>
                             </div>
      <a href="https://github.com/YangLiu9208/TSTDDs/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/YangLiu9208/TSTDDs"></a>
      <a href="https://github.com/YangLiu9208/TSTDDs/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/YangLiu9208/TSTDDs"></a>
      <a href="https://github.com/YangLiu9208/TSTDDs/issues"><img alt="GitHub issues" src="https://img.shields.io/github/issues/YangLiu9208/TSTDDs"></a>  
              </div>
            </div>              
       </ol>
            
<h3 class="mb-5">PhD Dissertation</h3>   
             <ol style="list-style-type:none">
                           <!-- 文章序列0 -->
            <div class="product-cell row align-items-center justify-content-center">
              <div class="col-lg-3 col-md-4 col-sm-12 col-xs-12">
                  <!--<li><video  muted="muted" width="200" controls= "autoPlay" onended="this.currentTime = 0; this.play();" autoplay="" src="img/XD.gif"> <source src="img/XD.mp4" type="video/mp4"></video></li>--> 
                <li><img class="product-cell-img" src="img/XD.gif"  style="width=240"></li>
              </div>
              <div class="product-cell-text col-lg-9 col-md-8 col-sm-12 col-xs-12">
                <b>Cross-domain Human Action Recognition via Transfer Learning (基于迁移学习的跨域人体行为识别研究)</b> 
                  <br><b>PhD Dissertation (博士学位论文), Xidian University (西安电子科技大学), June 30, 2019.</b></br> 
                  <b>Yang Liu</b><br>
                  <b>Supervisor：Prof. Zhaoyang Lu</b><br>
                
                                                                               <table class="imgtable">
                 <tr>
                    <td>
                        <p class="pub_link">[<a href= "https://cdmd.cnki.com.cn/Article/CDMD-10701-1020000550.htm" target="_blank">Paper</a>] 
                        [<a  onclick="display('PHD')" style="cursor:pointer;">BibTex</a>]
                        </p>
                        </td>
                        </tr>
                  </table>
                        <div>
                        <pre id="PHD" style="display: none;">
                        @phdthesis{刘阳2019基于迁移学习的跨域人体行为识别研究,
                          title={基于迁移学习的跨域人体行为识别研究},
                          author={刘阳},
                          year={2019},
                          school={西安电子科技大学}
                        }
                          </pre>
                         </div>
              </div>
            </div>
 </ol>
             
          
    </section>
    
 
    
    <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="activities">
      <div class="my-auto">
        <h3 class="mb-5">Academic services</h3>
          <ul>
              <b>Reviewer for Journals</b>
              <li>Advanced Science
              <li>IEEE Transactions on Pattern Analysis and Machine Intelligence
              <li>IEEE Transactions on Image Processing
              <li>IEEE Transactions on Neural Networks and Learning Systems
              <li>IEEE Transactions on Cybernetics   
              <li>IEEE Transactions on Multimedia
              <li>IEEE Transactions on Circuits and Systems for Video Technology
              <li>IEEE Transactions on Human-Machine Systems
              <li>IEEE Transactions on Mobile Computing
              <li>IEEE Signal Processing Letters
              <li>IEEE Robotics and Automation Letters
              <li>ACM Transactions on Multimedia Computing Communications and Applications
              <li>ACM Transactions on Information Systems
              <li>IET Computer Vision
              <li>Pattern Recognition
              <li>Information Fusion
              <li>Neural Networks    
              <li>Pattern Recognition Letters
              <li>Computer Vision and Image Understanding
              <li>Signal Processing: Image Communication
              <li>Machine Vision and Applications    
              <li>Journal of Visual Communication and Image Representation    
              <li>Visual Computer
          </ul>
          <ul>
              <b>Program Committee (PC)/Reviewer for Conferences</b>
              <li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
              <li>IEEE/CVF International Conference on Computer Vision (ICCV)
              <li>European Conference on Computer Vision (ECCV)
              <li>ACM Multimedia (ACM MM)
              <li>AAAI Conference on Artificial Intelligence (AAAI)
              <li>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
              <li>Computational Visual Media Conference (CVM)
              <li>Computer Graphics International (CGI)
              <li>Chinese Conference on Pattern Recognition and Computer Vision (PRCV)
              <li>ACM international joint conference on pervasive and ubiquitous computing (UbiComp)
              <li>IEEE International Semantic Web Conference (ISWC)
              <li>IEEE International Symposium on Circuits and Systems (ISCAS)
              <li>International AAAI Conference on Web and Social Media (ICWSM) 
              <li>CCF BigData Conference (CCF BigData)
          </ul>
                </div>
    </section> 
    
    
  </div>
  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/resume.min.js"></script>

</body>
<style>
  

.product-cell {
    /* width: 130%; */
    /* height: 90px; */
    margin-top: 15px;
  }

  .product-cell-img {
    /* float: left; */
    width: 240px;
    height: 120px;
  }

  .product-cell-text {
    display: inline;
  }

  .product-cell-num {
    /* float: left; */
    /* height: 70px; */
    color: black;
    margin-left: 15px;
    /* line-height: 70px; */
    text-align: center;
  }
</style>
</html>
